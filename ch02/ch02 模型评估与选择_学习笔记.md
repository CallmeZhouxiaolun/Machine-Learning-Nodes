## 第二章 模型评估与选择

1. **模型选择和评估方法(划分训练集和验证集合)**

   1.1 留出法：将数据集划分为两个互斥的集合，例如我们划分70%的数据为训练集， 30%的数据为验证集合。注意这里训练/验证集的划分要尽可能的保持数据分布的一致性，避免因数据划分过程中引入额外的偏差而对最终结果产生影响

   1.2 交叉验证：将数据集划分为k个大小相似的互斥子集，每个子集尽可能保持数据一致性，，每次使用k-1个子集的并集作为训练集，余下子集作为验证集。这样就可以进行k次训练和测试，以[k次结果均值作为最终的测试结果](https://zhuanlan.zhihu.com/p/30006720)

   1.3 留一法：交叉验证时，若k=m，即每一个样本为一个子集时候能尽量训练集规模的减少，但数据集较大时例如m为一百万，就以为这要训练一百万个模型，效率实在难以忍受

   * 上述方法存在问题在于：减少训练样本规模造成一定影响、实验效率较低。由此我们产生了下述方法

   1.4 自助法：以自助采样为基础，每次随机从样本集合D中挑选出一个放入D'中，然后再将该样本放回到初始集合中，使得该样本在下次采样时仍有可能被采样到，这个过程重复执行m次，就得到了包含个样本的数据集，我们以此为训练集，未在训练集中出现的样本组成验证集合。

   * 可以进行统计，样本在m次采样中始终不被采到的概率是：  
   <img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1582189256839.png" alt="1582189256839" style="zoom: 15%;" />
   

     * 即初始数据集D中约有36.8%的样本未出现在采样数据集中。

2. **性能度量**

   ​		获取模型之后，我们需要评估学习器的泛化性能。需要有衡量模型泛化性能的评价标注，这就是性能度量。在对比不同的模型能力时，使用不同的性能度量往往会导致不同的评判结果。

   * 例如回归任务最常用的性能度量是：均方误差

   2.1 **错误率和精度**

   2.2 **查准率、查全率、F1**

   ​		对于二分类任务，可将阳样例根据其真实类别和学习器预测类别的组合划分为：真正例、假正例、真反例、假反例，组成一下混淆矩阵： 
   <img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1582189811467.png" alt="1582189811467" style="zoom: 20%;" />  

   ​	1> 查准率：预测为真的结果中有多少真的正例  
   <img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1582189869788.png" alt="1582189869788" style="zoom:20%;" />  
   
   ​	2> 查全率：真正的正例中有多少被预测为正例  
   <img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1582189924565.png" alt="1582189924565" style="zoom:20%;" />
   
    * 查准率和查全率是一对矛盾的度量。一般来说查准率高时，查全率往往偏低，而查全率高时，查准率偏低

    * 我们可根据学习器的预测结果对样例进行排序，排在前面的是学习器任务‘最可能’是正例的样本，排在最后一个的是学习器认为‘最不可能’是正例的样本。按此顺序逐个把样本作为正例进行预测，则每次都可以计算查准率、查全率，以查准率、查全率为轴，可以得到查准率-查全率曲线（P-曲线）

      若一个学习器的P-R曲线被另一个学习器的曲线完全包住时，可断言后者的性能优于前者，若出现交叉时则难以区分优劣。

      * 此时一个比较好的评估方法是比较P-R曲线下的面积大小，但该面积难以估算
      * 平衡点：查准率==查全率的取值比较，但过于简单，

      所以更多时候采用F1值

   ​	3> F1: <img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1582190778882.png" alt="1582190778882" style="zoom:20%;" />

   <img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1582190880928.png" alt="1582190880928" style="zoom:20%;" />

   * 若有多个二分类混淆矩阵：例如，进行多次训练/测试，每次得到一个混淆矩阵，或者在多个数据集上训练测试，再或者执行多分类任务，两两类别组合对应一个混淆矩阵。我们需要在n个混淆矩阵上综合考虑查准率、查全率

* <img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1582191255012.png" alt="1582191255012" style="zoom:20%;" />

* <img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1582191284498.png" alt="1582191284498" style="zoom: 20%;" />
  <img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1582191313868.png" alt="1582191313868" style="zoom:20;" />

2.3 **ROC和AUC**

​		很多学习器会为测试样本产生一个实值或者概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正类，否则为反类。也就是说这个实值或概率预测结果的好坏，直接决定了学习器的泛化能力。实际上，根据这个实值或者概率预测结果，我们可以对测试样本进行排序，“最可能”是正例的排在最前面，“最不可能”是正例的排在最后面。这样分类的过程就相当于这个排序中以某个截断点将样本分为两部分，前一部分作为正例，后一部分作为负例。

​		在不同的应用任务中，我们可以根据任务需求来采用不同的截断点，若我们更重视查准率，则可采用排序中考前的位置进行截断，若更重视查全率，则可选择靠后的位置进行截断。因此排序本身的质量好坏，体现了综合考虑学习器在一般情况下泛化性能的好坏。ROC曲线就是从这个角度出发来研究学习器泛化性能的有力工具。

* 与P-R曲线相似，根据学习器的预测结果对样例进行排序，按此顺序逐个把样本作为正例进行预测，每次计算两个重要量的值，分别以他们为横纵坐标作图，就得到了ROC曲线。

* 与P-R曲线采用查准率、查全率作为纵、横轴不同，ROC曲线的纵轴是“真正例率”（TPR），横轴为“假正例率”（FPR）：

  <img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1582192863991.png" alt="1582192863991" style="zoom:20%;" />

  简单点说TRP就是真正为正例样本有多少被预测为正例， FPR就是真正为负例样本有多少预测为正例

<img src="https://github.com/CallmeZhouxiaolun/Machine-Learning-Nodes/blob/main/pic/1582193102202.png" alt="1582193102202" style="zoom:20%;" />

  * 在进行学习器的比较时，与P-R图相似，若一个学习器的二ROC曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者；若两个学习器的ROC曲线发生交叉，则难以断言。如果一定要进行比较，则较为合理的依据是比较ROC曲线下的面积， 即AUC

    <img src="https://github.com/CallmeZhouxiaolun/Machine-Learning-Nodes/blob/main/pic/1582193389169.png" alt="1582193389169" style="zoom:20%;" />

<img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1582193406072.png" alt="1582193406072" style="zoom:20%;" />

<img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1582194699486.png" alt="1582194699486" style="zoom:20%;" />

​					上述等式证明可见：[AUC=1-L证明](https://zhuanlan.zhihu.com/p/30006720)

​	2.4 **代价敏感错误率与代价曲线**

​		现实任务中常常会遇到这样的情况：不同类型的错误所造成的后果不同。以二分类任务为例：我们可根据任务的领域知识设定一个“代价矩阵”：

<img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1582261300955.png" alt="1582261300955" style="zoom:20%;" />

​		前面介绍的一些性能度量，大都隐式的假设了均等代价。在非均等代价下，我们所希望的不再是简单的最小化错误次数，而是希望最小化“总体代价”。设定代价敏感错误率为：

<img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1582261453417.png" alt="1582261453417" style="zoom:20%;" />

​		在非均等代价下，ROC曲线不能直接反映出学习器的期望总体代价，而“代价曲线”则可以达到该目的，代价曲线图的横轴是取值为[0,1]的正例概率代价：

<img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1582265405375.png" alt="1582265405375" style="zoom:20%;" />

  * 注意：书中将正例视为0，负例视为1。这里解释一下归一化 <img src="https://www.zhihu.com/equation?tex=p%2Acost_%7B01%7D%2B%281-p%29%2Acost_%7B10%7D" alt="[公式]" style="zoom:80%;" /> ，可以理解为把全部的样本全部判错的代价，即正的全判负，负的全判正的概率代价。没有分类器的期望代价会比这种情况下更糟糕的了，可以保证所得到的P(+)和下面的cost为[0,1]，因此采用这种方式进行归一化。

    纵轴是归一化代价，FPR定义为假正例率，FNR定义为假反例率。

<img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1582265867719.png" alt="1582265867719" style="zoom:20%;" />

  * 接下来我们解释一下采用该归一化代价的原因：

    根据对ROC曲线的理解，我们发现一个分类器对应一个阈值 $\eta$ ，每个阈值对应这ROC曲线上一点，即（FPR， TPR）。针对于单个分类器的期望代价，

     <img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1611314139(1).png" alt="[公式]" style="zoom:80%;" /> 

    该分类器的期望代价为：    
    $$
    \mathbb{E}[Cost] = \sum_{i=0}^{1}\sum_{j=0}^{1}c_{ij}Pr[decide\ H_j|H_i]Pr[H_i]  
    $$  
    其中， $c_{}ij$ 是代价矩阵，而 $Pr[decide\ H_j|H_i]$ 为条件概率，结合分类结果的混淆矩阵，可得到：  
    $$
    Pr[decide\ H_1|H_0] = FPR  
    Pr[decide\ H_1|H_1] = TPR    
    Pr[decide\ H_0|H_1] = FNR    
    Pr[decide\ H_0|H_0] = TNR    
    $$    
      
    而 $Pr[H_i]$ 为先验概率，其中 $Pr[H_1]=p$ 即为正例概率，负例概率为 $Pr[H_0] =1-Pr[H_1]=1-p$      
    
    将条件概率和先验概率带入到期望代价 $\mathbb{E}[Cost]$ 中，同时 $c_{00}\ c_{11}$ 都等于0，计算可得：          
    $$
    \mathbb{E}[Cost] = c_{01}*(1-p)*FPR + c_{10}*p*FNR  
    $$      
      
    * 将上式与归一化代价的分字进行比较，区别在于周老师书中将0视为正例，1视为反例，这里根据习惯将1视为正例，将0视为反例，其实质相同。
    * 因此代价曲线实质为正例概率代价和期望代价的关系图
    
* 进一步分析x、y函数，我们可以发现： $y=FNR*x + FPR*(1-x)$ 。针对一个分类器，对应于ROC上的一个点（FNR，TPR），可计算出FNR，然后在代价平面上绘制一条 (0, FPR)到(1,FNR)的线段。线段下的面积即表示了该条件下的期望总体代价。

  注意：这里即为该分类器下针对所有可能的P的期望总体代价。

* 如此将ROC曲线上的每个点转化为代价平面的一条线段，然后取所有线段的下界，围成的面积即为所有条件下学习器的期望总体代价。

  注意：这里的所有条件，指的是学习器在所有阈值上产生的所有分类器在可能的P下的期望总体代价

  * 接下来我们讨论所有条件下学习器的期望总体代价：     
  $$
  C_{l}(P) = \inf_{f\in F}\{FNR*P+FPR*(1-P)\}\ P\in[0,1]  
  $$    

  * 假设P服从U(0,1)的均匀分布，概率密度为f(P)，则期望总体代价为：      
    $$
    \begin{align}    
    \mathbb{E}_{P\sim U(0,1)}  &= \int_{0}^{1}U(P;0,1)C_{l}dP  
    &= \int_{0}^{1}C_{l}(P)dP  
    &= A_{l}  
    \end{align}  
    $$
    由此可以证明，在先验概率P符合0-1的均匀分布下，学习器$l$对应的所有分类器、所有P下的期望总体代价等于代价曲线所围成的面积。

3. **比较检验**

   

4. **偏差和方差**

   算法的期望泛化误差可进一步划分为：偏差、方差、噪声

   * 首先介绍一下期望值的含义： 期望值的含义是指在**同样的条件**下**重复多次**随机试验，得到的所有可能状态的平均结果 。对于机器学习来说，我们选定一种算法(选定超参数)，以及设置一个固定的训练集大小，以相同上述条件为标准即为特定模型。

   * 然后每次训练时从样本空间中选择一批样本作为训练集，但每次都随机抽取不同的样本，这样重复进行多次训练。每次训练会得到一个具体的模型，每个具体模型对同一个未见过的样本进行预测可以得到预测值。不断重复训练和预测，就能得到一系列预测值，根据样本和这些预测值计算出方差和偏差，就可以帮助我们考察该特定模型的预测误差的期望值，也就能衡量该特定模型的性能。对比多个特定模型的误差的期望值，可以帮助我们选择合适的模型。 

   * 接下来举个例子：

     设置真实模型 $f(x)=x+2sin(1.5x)$，样本值 y 就在真实值的基础上叠加一个随机噪音 N(0,0.2)。 

     <img src="https://pic1.zhimg.com/v2-b0580e8e74d970d3ff5b7f20f82d6d94_r.jpg" alt="preview" style="zoom:50%;" /> 

     * 使用线性函数来构建模型，训练样本来自随机采集的一组 y，经过多次重复，可以得到一系列具体的线性模型，如下图中那一组聚集在一起的黑色直线所示，其中间有一条红色线是这一组线性函数的平均（期望值）。这就是特定模型（线性函数）在同样条件下（每次取20个样本点）重复多次（得到50个线性函数）。 

       <img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1582282073517.png" alt="1582282073517" style="zoom:50%;" />

       * 这里针对特定x值，黑色直线在x值上一系列分布反映了它的方差，而 50个预测的期望值（红线）与真实值 f(x) 之间的距离体现了**偏差（Bias）** ，样本值y与真实值之间的差异为噪声。

   4.1 **简单介绍一下偏差、方差、噪声**

   *  **Bias**是用**所有可能的训练数据集**训练出的**所有模型**的输出的**平均值**与**真实模型**的输出值之间的差异。 
   *   **Variance**是**不同的训练数据集训练出的模型**输出值之间的差异。 
   *   **噪声**的存在是学习算法所无法解决的问题，数据的质量决定了学习的上限。假设在数据已经给定的情况下，此时上限已定，我们要做的就是尽可能的接近这个上限。 

   4.2 **以数学公式定义偏差、方差、噪声**

   ​	首先定义数学符号：

    <img src="https://pic3.zhimg.com/v2-75d3929ab76068d100988df5157402be_r.jpg" alt="preview" style="zoom: 80%;" /> 

   以回归任务为例：学习算法的**期望预测**为：
   $$
   \overline{f}(x) = \mathbb{E}_{D}[f(x,D)]
   $$
   注意：这里的期望就是针对不同数据集D，算法训练的所有模型对下的预测值取期望，即平均预测

   * 使用样本数相同的不同训练集产生的**方差**为：
     $$
     var(x) = \mathbb{E}_{D}[(f(x;D)-\overline f(x))^2]
     $$
     方差度量的是： **同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响。** 

   * **偏差**：期望输出与真实标记的差别：
     $$
     bias^{2}(x) = (\overline f(x)-y)^{2}
     $$
     偏差度量的是： **学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力。** 

   * 噪声：
     $$
     \varepsilon^{2} = \mathbb E_{D}[(y_{D}-y)^2]
     $$
     噪声含义： **表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。** 

   综上：偏差度量的是特定模型(算法、参数、训练集选定)的拟合能力，方差度量的是同一模型在不同数据集上的稳定性

   4.3 **泛化误差与偏差、方差、噪声的关系**

   ​	为便于讨论，假定噪声期望为0， 即  $ \mathbb E_{D}[y_{D}-y]=0 $，通过简单的多项式展开合并，可对算法的期望泛化误差进行分解：

   <img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1582283465273.png" alt="1582283465273" style="zoom:20%;" />

   关于第三、第六个等式的化简：

   ​		<img src="https://github.com/CallmeZhouxiaolun/Machine-Learning-Nodes/blob/main/pic/1582284476505.png" alt="1582284476505" style="zoom:15%;" />

   

     * 偏差-方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及任务本身的难度所共同觉得的。给定学习任务，为了取得好的泛化性能，则需使偏差偏小，即能充分拟合数据，并且使方差较小，即使的数据扰动产生的影响小。

   4.4 **偏差和方差的图形解释**

     

    <img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1611314035(1).png" alt="preview" style="zoom: 50%;" /> 

   4.5 **偏差方差窘境**

   <img src="https://cdn.jsdelivr.net/gh/CallmeZhouxiaolun/Machine-Learning-Nodes@main/pic/1582285269162.png" alt="1582285269162" style="zoom:20%;" />

   4.6 **偏差、方差和模型复杂度的关系**

    复杂度高的模型通常对训练数据有很好的拟合能力，但是对测试数据就不一定了。而复杂度太低的模型又不能很好的拟合训练数据，更不能很好的拟合测试数据。因此，模型复杂度和模型偏差和方差具有如下图所示关系。 

   ​		 ![preview](https://pic4.zhimg.com/v2-1c8804f2885d07958a55c50164e74b43_r.jpg) 

   4.7  **偏差、方差与bagging、boosting的关系**

   ​		Bagging算法是对训练样本进行采样，产生出若干不同的子集，再从每个数据子集中训练出一个分类器，取这些分类器的平均，所以是降低模型的方差（variance）。Bagging算法和Random Forest这种并行算法都有这个效果。

   ​		Boosting则是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行权重调整，所以随着迭代不断进行，误差会越来越小，所以模型的偏差（bias）会不断降低。

   4.8 **如何解决偏差、方差问题？**

   ​		**整体思路：**首先，要知道偏差和方差是无法完全避免的，只能尽量减少其影响。
   ​	（1）在避免偏差时，**需尽量选择正确的模型**，一个非线性问题而我们一直用线性模型去解决，那无论如何，高偏差是无法避免的。
   ​	（2）有了正确的模型，**我们还要慎重选择数据集的大小**，通常数据集越大越好，但大到数据集已经对整体所有数据有了一定的代表性后，再多的数据已经不能提升模型了，反而会带来计算量的增加。而训练数据太小一定是不好的，这会带来过拟合，模型复杂度太高，方差很大，不同数据集训练出来的模型变化非常大。
   ​	（3）最后，**要选择合适的模型复杂度**，复杂度高的模型通常对训练数据有很好的拟合能力。

   ​	**针对偏差和方差的思路：**
   ​	**偏差：**实际上也可以称为避免欠拟合。
   ​	1、寻找更好的特征 -- 具有代表性。
   ​	2、用更多的特征 -- 增大输入向量的维度。（增加模型复杂度）
   ​	**方差：**避免过拟合
   ​	1、增大数据集合 -- 使用更多的数据，减少数据扰动所造成的影响
   ​	2、减少数据特征 -- 减少数据维度，减少模型复杂度
   ​	3、正则化方法
   ​	4、交叉验证法
 
    * 注：偏差、方差参考以下资料：

      [偏差Bias和方差Variance——机器学习中的模型选择](https://zhuanlan.zhihu.com/p/44872686)

      [偏差（Bias）与方差（Variance）](https://zhuanlan.zhihu.com/p/38853908)	
